{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Necessary Libraries"
      ],
      "metadata": {
        "id": "WK15Jqb1H2Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import zipfile\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "0fKLJhiLF9mG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip the file"
      ],
      "metadata": {
        "id": "ud-xjUxFH_e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/archive.zip\"\n",
        "extract_path = \"/content/sentiment_data\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List extracted files\n",
        "\n",
        "file_list = os.listdir(extract_path)"
      ],
      "metadata": {
        "id": "zR6x_84cXlD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a sentiment analysis classifier and evaluate the model"
      ],
      "metadata": {
        "id": "Eb1u1xrCIwzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv('/content/sentiment_data/train.csv',  encoding = 'latin-1')\n",
        "\n",
        "# Check the first few rows to understand the data structure\n",
        "print(\"Dataset preview:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for class distribution\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "# Map sentiment labels to numbers if needed\n",
        "sentiment_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
        "df['sentiment_label'] = df['sentiment'].map(sentiment_map)\n",
        "\n",
        "# Split data into train and test sets in a ratio 80:20\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'],\n",
        "    df['sentiment_label'],\n",
        "    test_size=0.2,\n",
        "    random_state=seed,\n",
        "    stratify=df['sentiment_label']\n",
        ")\n",
        "\n",
        "# Step 3: Create a PyTorch Dataset\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        label = self.labels.iloc[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Step 4: Load pre-trained transformer model and tokenizer\n",
        "model_name = \"distilbert-base-uncased\"  # A smaller, faster version of BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3  # positive, neutral, negative\n",
        ")\n",
        "\n",
        "# Step 5: Create data loaders\n",
        "train_dataset = SentimentDataset(X_train, y_train, tokenizer)\n",
        "test_dataset = SentimentDataset(X_test, y_test, tokenizer)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Step 6: Set up training parameters\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 3\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# Create learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Step 7: Training loop\n",
        "def train():\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Clear gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    return avg_train_loss\n",
        "\n",
        "# Step 8: Evaluation function\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            # Get predictions\n",
        "            logits = outputs.logits\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "            # Store predictions and true labels\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            true_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    # Convert indices back to sentiment names for the classification report\n",
        "    sentiment_names = {v: k for k, v in sentiment_map.items()}\n",
        "    y_pred = [sentiment_names[pred] for pred in predictions]\n",
        "    y_true = [sentiment_names[label] for label in true_labels]\n",
        "\n",
        "    # Return classification report\n",
        "    return classification_report(y_true, y_pred)\n",
        "\n",
        "# Step 9: Train and evaluate the model\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    avg_train_loss = train()\n",
        "    print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "# Final evaluation and classification report\n",
        "print(\"\\nEvaluating the model...\")\n",
        "report = evaluate()\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./sentiment_transformer_model\")\n",
        "tokenizer.save_pretrained(\"./sentiment_transformer_model\")\n",
        "print(\"\\nModel saved to ./sentiment_transformer_model\")"
      ],
      "metadata": {
        "id": "yZCz2TvgF7TG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17674d5-82d3-4d2f-fd58-8767ca6ca4ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset preview:\n",
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment Time of Tweet Age of User  \\\n",
            "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
            "1                             Sooo SAD  negative          noon       21-30   \n",
            "2                          bullying me  negative         night       31-45   \n",
            "3                       leave me alone  negative       morning       46-60   \n",
            "4                        Sons of ****,  negative          noon       60-70   \n",
            "\n",
            "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
            "0  Afghanistan          38928346         652860.0               60  \n",
            "1      Albania           2877797          27400.0              105  \n",
            "2      Algeria          43851044        2381740.0               18  \n",
            "3      Andorra             77265            470.0              164  \n",
            "4       Angola          32866272        1246700.0               26  \n",
            "\n",
            "Class distribution:\n",
            "sentiment\n",
            "neutral     11118\n",
            "positive     8582\n",
            "negative     7781\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Starting training...\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1374/1374 [04:03<00:00,  5.63it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average training loss: 0.5902\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1374/1374 [04:02<00:00,  5.67it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average training loss: 0.4219\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1374/1374 [04:02<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.3151\n",
            "\n",
            "Evaluating the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 344/344 [00:20<00:00, 16.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.81      0.80      0.80      1556\n",
            "     neutral       0.77      0.77      0.77      2224\n",
            "    positive       0.83      0.84      0.83      1717\n",
            "\n",
            "    accuracy                           0.80      5497\n",
            "   macro avg       0.80      0.80      0.80      5497\n",
            "weighted avg       0.80      0.80      0.80      5497\n",
            "\n",
            "\n",
            "Model saved to ./sentiment_transformer_model\n"
          ]
        }
      ]
    }
  ]
}