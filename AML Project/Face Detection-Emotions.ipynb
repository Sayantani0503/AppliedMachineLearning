{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688b46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import display, Image\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2af17",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m         cv2.destroyAllWindows()\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Run the face detection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mwebcam_face_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mwebcam_face_detection\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     26\u001b[39m gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Detect faces\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m faces = \u001b[43mface_cascade\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaleFactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminNeighbors\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminSize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Draw rectangles around detected faces\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m faces:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Detection\n",
    "\n",
    "def webcam_face_detection():\n",
    "    # Load the pre-trained Haar Cascade face detector\n",
    "    face_cascade = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "    )\n",
    "    \n",
    "    # Initialize webcam (0 = default camera)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Check if webcam opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Read frame from webcam\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Can't receive frame\")\n",
    "                break\n",
    "            \n",
    "            # Convert to grayscale for face detection\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Detect faces\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30)\n",
    "            )\n",
    "            # Draw rectangles around detected faces\n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('Face Detection - Press Q to Quit', frame)\n",
    "            \n",
    "            # Break loop when 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "    finally:\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Run the face detection\n",
    "webcam_face_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ee57e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.0\n",
      "0.0.93\n",
      "8.0.4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import deepface\n",
    "import ipywidgets\n",
    "print(cv2.__version__)\n",
    "print(deepface.__version__)\n",
    "print(ipywidgets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd9ac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:07<00:00,  2.47s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182316_Sad.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:02<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182319_Neutral.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182321_Neutral.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182323_Neutral.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182326_Neutral.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182328_Happy.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182330_Happy.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182332_Fear.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182334_Sad.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182336_Neutral.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182338_Happy.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182340_Happy.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182343_Happy.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182345_Neutral.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182347_Neutral.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182349_Neutral.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182351_Sad.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 3/3 [00:01<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: captured_faces/face_20250507_182353_Happy.jpg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "\n",
    "def detect_faces_and_attributes():\n",
    "    # Create a directory to save images\n",
    "    output_dir = \"captured_faces\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Load Haar Cascade for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    # Variables for attribute detection\n",
    "    frame_skip = 10  # Process attributes every 10 frames\n",
    "    frame_count = 0\n",
    "    last_emotion = \"None\"\n",
    "    last_emotion_score = 0.0\n",
    "    last_age = \"Unknown\"\n",
    "    last_gender = \"Unknown\"\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame\")\n",
    "                break\n",
    "            \n",
    "            # Convert to grayscale for face detection\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Detect faces\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30)\n",
    "            )\n",
    "            \n",
    "            # Create a copy of the frame to annotate\n",
    "            annotated_frame = frame.copy()\n",
    "            \n",
    "            # Process each face found\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Draw rectangle around face\n",
    "                color = (0, 255, 0)  # Green\n",
    "                cv2.rectangle(annotated_frame, (x, y), (x+w, y+h), color, 2)\n",
    "                \n",
    "                # Attribute detection (emotion, age, gender) every frame_skip frames\n",
    "                if frame_count % frame_skip == 0:\n",
    "                    try:\n",
    "                        # Extract face ROI\n",
    "                        face_roi = frame[y:y+h, x:x+w]\n",
    "                        if face_roi.size > 0:\n",
    "                            # Analyze emotion, age, and gender\n",
    "                            analysis = DeepFace.analyze(\n",
    "                                img_path=face_roi,\n",
    "                                actions=['emotion', 'age', 'gender'],\n",
    "                                detector_backend='opencv',\n",
    "                                enforce_detection=False\n",
    "                            )\n",
    "                            last_emotion = analysis[0]['dominant_emotion'].capitalize()\n",
    "                            last_emotion_score = analysis[0]['emotion'][last_emotion.lower()]\n",
    "                            last_age = analysis[0]['age']\n",
    "                            last_gender = analysis[0]['dominant_gender']\n",
    "                        else:\n",
    "                            last_emotion = \"No Face\"\n",
    "                            last_emotion_score = 0.0\n",
    "                            last_age = \"Unknown\"\n",
    "                            last_gender = \"Unknown\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"Attribute detection error: {e}\")\n",
    "                        last_emotion = \"Error\"\n",
    "                        last_emotion_score = 0.0\n",
    "                        last_age = \"Unknown\"\n",
    "                        last_gender = \"Unknown\"\n",
    "                \n",
    "                # Display attributes\n",
    "                emotion_text = f\"{last_emotion}: {last_emotion_score:.1f}%\"\n",
    "                age_gender_text = f\"Age: {last_age}, Gender: {last_gender}\"\n",
    "                cv2.putText(annotated_frame, emotion_text, (x, y-40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "                cv2.putText(annotated_frame, age_gender_text, (x, y-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "                \n",
    "                # Save the frame with detected face and text\n",
    "                if frame_count % frame_skip == 0:\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    filename = f\"{output_dir}/face_{timestamp}_{last_emotion}.jpg\"\n",
    "                    cv2.imwrite(filename, annotated_frame)\n",
    "                    print(f\"Saved image: {filename}\")\n",
    "            \n",
    "            # Display frame count and instructions\n",
    "            cv2.putText(annotated_frame, f\"Faces: {len(faces)}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(annotated_frame, \"Press Q to Quit\", (10, annotated_frame.shape[0]-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show the frame\n",
    "            cv2.imshow('Face & Attribute Detection', annotated_frame)\n",
    "            \n",
    "            # Exit on 'q' key\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Run the detection\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        detect_faces_and_attributes()\n",
    "    except ImportError as e:\n",
    "        print(f\"Import error: {e}\")\n",
    "        print(\"Installing required packages...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\", \"deepface\", \"numpy\", \"--user\"])\n",
    "        print(\"Packages installed. Please run the program again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
