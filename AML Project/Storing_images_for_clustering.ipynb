{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29cffc78-6526-4d0e-9724-4fdce2436102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/sunsmarterjie/yolov12.git\n",
    "# %cd yolov12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58183981-50c2-4024-8e5a-2fee702da122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt\n",
    "# %pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36d6a46-df24-4e42-9e75-291d2153d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/himanshu/yolov12\n"
     ]
    }
   ],
   "source": [
    "%cd yolov12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe9eafe-5070-4767-ae57-2d1d0b9162e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/python311_env/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention is not available on this device. Using scaled_dot_product_attention instead.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938ec563-bca3-46c8-8d7a-f1cef2412a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/himanshu/Downloads/YoloTestImg.jpeg: 256x320 6 faces, 73.1ms\n",
      "Speed: 2.8ms preprocess, 73.1ms inference, 3.2ms postprocess per image at shape (1, 3, 256, 320)\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/himanshu/Downloads/face_detector_yolov12.pt'\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = YOLO(model_path)\n",
    "# test_image = '/home/himanshu/Downloads/YoloTestImg.jpeg'\n",
    "# result = model(test_image)\n",
    "# result[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "455b247f-7f39-4b16-8e94-ad3f249b167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in ./python311_env/venv/lib/python3.11/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./python311_env/venv/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfafee1d-6b4d-4ccc-8550-ccbd698a8a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Press 1–9 to start capturing for human1 to human9\n",
      "[INFO] Press 'q' to quit\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113222_364270.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113222_683675.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113222_993776.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113223_314708.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113223_633203.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113223_952743.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113224_263214.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113224_588006.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113224_903143.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113225_218531.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113225_529604.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113225_842762.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113226_154271.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113226_470304.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113226_796177.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113227_114116.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113227_438209.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113227_745081.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113228_060625.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113228_371737.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113228_683258.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113229_000697.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113229_315385.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113229_636159.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113229_947558.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113230_262411.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113230_590899.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113230_899403.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113231_210287.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113231_529372.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113231_841070.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113232_161643.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113232_476420.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113232_796348.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113233_113625.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113233_424507.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113233_751158.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113234_056285.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113234_370657.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113234_679026.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113235_021753.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113235_338768.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113235_644025.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113235_951685.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113236_254669.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113236_557906.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113236_866396.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113237_182891.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113237_491425.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113237_802273.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113238_123050.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113238_437355.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113238_742518.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113239_051896.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113239_358583.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113239_677471.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113239_979241.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113240_288818.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113240_600455.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113240_911892.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113241_226931.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113241_568483.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113241_879777.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113242_195298.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113242_512836.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113242_812523.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113243_151285.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113243_462255.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113243_803623.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113244_125706.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113244_428178.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113244_746631.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113245_032781.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113245_349065.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113245_654555.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113245_972631.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113246_284910.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113246_627358.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113246_914633.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113247_227938.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113247_551833.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113247_857013.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113248_162958.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113248_496714.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113248_805455.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113249_120782.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113249_424720.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113249_732767.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113250_046717.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113250_351722.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113250_665464.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113250_975255.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113251_288750.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113251_622412.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113251_921993.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113252_231791.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113252_546938.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113252_866458.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113253_172433.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113253_490320.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113258_136504.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113258_427623.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113258_751582.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113259_057088.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113259_395760.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113259_708832.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113300_016409.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113300_335646.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113300_671194.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113300_985032.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113301_316688.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113301_644790.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113301_962494.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113302_279335.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113302_588361.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113302_927418.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113303_260495.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113303_570946.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113303_887693.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113304_242090.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113304_543357.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113304_856604.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113305_170437.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113305_494194.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113305_811713.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113306_122894.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113306_434956.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113306_750674.jpg\n",
      "[SAVED] human_faces_dataset/human1/img_20250507_113307_066113.jpg\n",
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO  # Make sure ultralytics is installed\n",
    "import string\n",
    "import time\n",
    "\n",
    "\n",
    "# Add at the start of the loop\n",
    "current_time = time.time()\n",
    "\n",
    "# Add this outside the loop to store the last time an image was captured\n",
    "last_capture_time = 0\n",
    "capture_interval = 0.3  # seconds between captures (e.g., 0.5s = 500ms)\n",
    "\n",
    "\n",
    "# Load your trained YOLOv12 model\n",
    "model_path = \"/home/himanshu/Downloads/face_detector_yolov12.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "# Base directory where data will be saved\n",
    "dataset_dir = \"human_faces_dataset\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Settings\n",
    "images_per_class = 100\n",
    "capture_key_map = {str(i): f\"human{i}\" for i in range(1, 10)}  # '1' → 'horse1', etc.\n",
    "\n",
    "print(\"[INFO] Press 1–9 to start capturing for human1 to human9\")\n",
    "print(\"[INFO] Press 'q' to quit\")\n",
    "\n",
    "current_label = None\n",
    "image_count = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect horse faces\n",
    "        current_time = time.time()\n",
    "        results = model.predict(source=frame, device='cpu', conf=0.6, verbose=False)\n",
    "        boxes = results[0].boxes\n",
    "\n",
    "        # Draw boxes and current capture state\n",
    "        adjusted_boxes = None\n",
    "        for box in boxes:\n",
    "            # x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            h, w, _ = frame.shape\n",
    "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "\n",
    "            # Expand box: top +5%, left/right +2%\n",
    "            box_width = x2 - x1\n",
    "            box_height = y2 - y1\n",
    "                \n",
    "            x1 = max(0, int(x1 - 0.04 * box_width))\n",
    "            x2 = min(w, int(x2 + 0.04 * box_width))\n",
    "            y1 = max(0, int(y1 - 0.15 * box_height))\n",
    "            y2 = min(h, int(y2))  # bottom stays same\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            if adjusted_boxes is None:\n",
    "                adjusted_boxes = (x1 , y1, x2 , y2)\n",
    "\n",
    "        if current_label and image_count < images_per_class:\n",
    "            # Save the first detected horse face\n",
    "            if len(boxes) > 0 and (current_time - last_capture_time) > capture_interval:\n",
    "                x1, y1, x2, y2 = adjusted_boxes\n",
    "                face_crop = frame[y1:y2, x1:x2]\n",
    "                \n",
    "                label_dir = os.path.join(dataset_dir, current_label)\n",
    "                os.makedirs(label_dir, exist_ok=True)\n",
    "                \n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "                filename = f\"{label_dir}/img_{timestamp}.jpg\"\n",
    "                cv2.imwrite(filename, face_crop)\n",
    "                print(f\"[SAVED] {filename}\")\n",
    "                image_count += 1\n",
    "                last_capture_time = current_time  # update last capture time                \n",
    "\n",
    "        # Display info\n",
    "        status_text = f\"Capturing: {current_label or 'None'} ({image_count}/{images_per_class})\"\n",
    "        cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1, (0, 255, 255), 2)\n",
    "\n",
    "        # Show video\n",
    "        cv2.imshow(\"Human Face Collector\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord('q'):\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "\n",
    "        elif chr(key) in capture_key_map.keys():\n",
    "            current_label = capture_key_map[chr(key)]\n",
    "            image_count = 0\n",
    "            # print(f\"[INFO] Started capturing for: {current_label}\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393db490-81d8-44ee-b10a-54f58ecd8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## from torchvision import models, transforms\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "\n",
    "# # Load pretrained ResNet50 and remove final classification layer\n",
    "# resnet = models.resnet50(pretrained=True)\n",
    "# feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC\n",
    "# feature_extractor.eval()  # inference mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "872dbec2-3d9f-4850-ae69-b76833c2e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=[0.485, 0.456, 0.406], \n",
    "#         std=[0.229, 0.224, 0.225])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c9b9285-69ad-4c5e-a2f6-d69d367118f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# def extract_embedding(img_path):\n",
    "#     image = Image.open(img_path).convert('RGB')\n",
    "#     image = transform(image).unsqueeze(0)  # Add batch dim\n",
    "#     with torch.no_grad():\n",
    "#         embedding = feature_extractor(image).squeeze().numpy()\n",
    "#         embedding = embedding / np.linalg.norm(embedding)  # normalize\n",
    "#     return embedding\n",
    "\n",
    "# # Dictionary to store embeddings\n",
    "# human_embeddings = {}\n",
    "\n",
    "# dataset_path = \"human_faces_dataset\"\n",
    "# for human_name in os.listdir(dataset_path):\n",
    "#     class_dir = os.path.join(dataset_path, human_name)\n",
    "#     if not os.path.isdir(class_dir):\n",
    "#         continue\n",
    "\n",
    "#     emb_list = []\n",
    "#     for img_file in os.listdir(class_dir):\n",
    "#         img_path = os.path.join(class_dir, img_file)\n",
    "#         emb = extract_embedding(img_path)\n",
    "#         emb_list.append(emb)\n",
    "    \n",
    "#     # average embedding for that horse\n",
    "#     human_embeddings[human_name] = np.mean(emb_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf79858-6f13-4fd3-a18c-f68179ebfbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "\n",
    "# EMBEDDING_FILE = 'face_embeddings.pkl'\n",
    "\n",
    "# def save_embeddings(embeddings_dict):\n",
    "#     with open(EMBEDDING_FILE, 'wb') as f:\n",
    "#         pickle.dump(human_embeddings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f9b84-e136-4244-b38d-43f2663f1e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ab6afb2-2736-4e5e-ba87-6b1ec56eec79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/python311_env/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/himanshu/python311_env/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting face recognition... Press 'q' to quit.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'classifiers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    104\u001b[39m face_crop = frame[y1:y2, x1:x2]\n\u001b[32m    105\u001b[39m emb = get_face_embedding(face_crop)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m name = \u001b[43mrecognize_face_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m cv2.rectangle(frame, (x1, y1), (x2, y2), (\u001b[32m0\u001b[39m,\u001b[32m255\u001b[39m,\u001b[32m0\u001b[39m), \u001b[32m2\u001b[39m)\n\u001b[32m    109\u001b[39m cv2.putText(frame, name, (x1, y1-\u001b[32m10\u001b[39m), cv2.FONT_HERSHEY_SIMPLEX, \u001b[32m0.9\u001b[39m, (\u001b[32m0\u001b[39m,\u001b[32m255\u001b[39m,\u001b[32m0\u001b[39m), \u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mrecognize_face_binary\u001b[39m\u001b[34m(embedding, model_dir, threshold)\u001b[39m\n\u001b[32m      2\u001b[39m best_match = \u001b[33m\"\u001b[39m\u001b[33mUnknown\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m best_score = threshold\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fname.endswith(\u001b[33m'\u001b[39m\u001b[33m_clf.pkl\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      7\u001b[39m         name = fname.replace(\u001b[33m'\u001b[39m\u001b[33m_clf.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'classifiers'"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# from torchvision import models, transforms\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# # ======= SETUP =======\n",
    "# # Preprocessing transform for ResNet-50\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# # Load pretrained ResNet-50\n",
    "# resnet = models.resnet50(pretrained=True)\n",
    "# resnet.fc = torch.nn.Identity()\n",
    "# resnet.eval()\n",
    "\n",
    "# # Load saved embeddings\n",
    "# with open('face_embeddings.pkl', 'rb') as f:\n",
    "#     known_embeddings = pickle.load(f)\n",
    "\n",
    "# # ======= FACE EMBEDDING FUNCTION =======\n",
    "# def get_face_embedding(face_image):\n",
    "#     img = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "#     input_tensor = transform(img).unsqueeze(0)\n",
    "#     with torch.no_grad():\n",
    "#         embedding = resnet(input_tensor).squeeze(0).numpy()\n",
    "#         embedding = embedding / np.linalg.norm(embedding)\n",
    "#     return embedding\n",
    "\n",
    "# # ======= FACE RECOGNITION FUNCTION =======\n",
    "# def recognize_face(embedding):\n",
    "#     best_match = \"Unknown\"\n",
    "#     best_score = 0.0\n",
    "#     for name, stored_emb in known_embeddings.items():\n",
    "#         score = cosine_similarity([embedding], [stored_emb])[0][0]\n",
    "#         if score > best_score:\n",
    "#             best_score = score\n",
    "#             best_match = name\n",
    "#     return best_match if best_score > 0.15 else \"Unknown\"\n",
    "\n",
    "# # ======= PLACEHOLDER: Your YOLOv12 face detector =======\n",
    "# # Replace this with actual face detection from YOLOv12\n",
    "# def detect_faces_yolov12(frame, conf_threshold=0.6):\n",
    "#     results = model.predict(source=frame, device='cpu', conf=conf_threshold, verbose=False)\n",
    "#     boxes = results[0].boxes\n",
    "\n",
    "#     detected_faces = []\n",
    "#     h, w, _ = frame.shape\n",
    "\n",
    "#     for box in boxes:\n",
    "#         x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "\n",
    "#         # Expand box: top +15%, sides +4%\n",
    "#         box_width = x2 - x1\n",
    "#         box_height = y2 - y1\n",
    "\n",
    "#         x1 = max(0, int(x1 - 0.04 * box_width))\n",
    "#         x2 = min(w, int(x2 + 0.04 * box_width))\n",
    "#         y1 = max(0, int(y1 - 0.15 * box_height))\n",
    "#         y2 = min(h, int(y2))  # bottom stays same\n",
    "\n",
    "#         # Convert to (x, y, w, h)\n",
    "#         x = x1\n",
    "#         y = y1\n",
    "#         w_box = x2 - x1\n",
    "#         h_box = y2 - y1\n",
    "\n",
    "#         detected_faces.append((x, y, w_box, h_box))\n",
    "\n",
    "#     return detected_faces\n",
    "\n",
    "\n",
    "# # ======= REAL-TIME WEBCAM LOOP =======\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# print(\"Starting face recognition... Press 'q' to quit.\")\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Mirror the frame (flip horizontally)\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "\n",
    "#     # Detect faces\n",
    "#     boxes = detect_faces(frame)\n",
    "\n",
    "#     for (x, y, w, h) in boxes:\n",
    "#         # Adjust box size (5% top, 2% sides)\n",
    "#         x1 = int(x - 0.02 * w)\n",
    "#         y1 = int(y - 0.05 * h)\n",
    "#         x2 = int(x + w + 0.02 * w)\n",
    "#         y2 = int(y + h + 0.05 * h)\n",
    "#         x1, y1 = max(0, x1), max(0, y1)\n",
    "#         x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "\n",
    "#         face_crop = frame[y1:y2, x1:x2]\n",
    "#         emb = get_face_embedding(face_crop)\n",
    "#         name = recognize_face_binary(emb)\n",
    "\n",
    "#         cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "#         cv2.putText(frame, name, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "\n",
    "#     cv2.imshow(\"Face Recognition\", frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5999c627-3499-4da4-b1bf-ad0ac5b48102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# def train_binary_classifiers(embedding_file='face_embeddings.pkl', model_dir='classifiers'):\n",
    "#     os.makedirs(model_dir, exist_ok=True)\n",
    "#     with open(embedding_file, 'rb') as f:\n",
    "#         embeddings = pickle.load(f)\n",
    "\n",
    "#     names = list(embeddings.keys())\n",
    "#     X = np.array(list(embeddings.values()))\n",
    "#     for i, name in enumerate(names):\n",
    "#         y = np.array([1 if j == i else 0 for j in range(len(names))])\n",
    "#         clf = LogisticRegression(max_iter=1000)\n",
    "#         clf.fit(X, y)\n",
    "\n",
    "#         model_path = os.path.join(model_dir, f\"{name}_clf.pkl\")\n",
    "#         with open(model_path, 'wb') as f_out:\n",
    "#             pickle.dump(clf, f_out)\n",
    "#         print(f\"Saved classifier for {name} to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4f04715-ebef-4e27-b7e0-ef07637972c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recognize_face_binary(embedding, model_dir='classifiers', threshold=0.6):\n",
    "#     best_match = \"Unknown\"\n",
    "#     best_score = threshold\n",
    "\n",
    "#     for fname in os.listdir(model_dir):\n",
    "#         if fname.endswith('_clf.pkl'):\n",
    "#             name = fname.replace('_clf.pkl', '')\n",
    "#             with open(os.path.join(model_dir, fname), 'rb') as f:\n",
    "#                 clf = pickle.load(f)\n",
    "#             score = clf.predict_proba([embedding])[0][1]\n",
    "#             if score > best_score:\n",
    "#                 best_score = score\n",
    "#                 best_match = name\n",
    "#     return best_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d601b214-7b09-4a4d-831b-bdc646a17c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# from torchvision import models, transforms\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# # ========== CONFIG ==========\n",
    "# EMBEDDING_FILE = 'face_embeddings.pkl'\n",
    "# CLASSIFIER_DIR = 'classifiers'\n",
    "# THRESHOLD = 0.6  # confidence threshold for binary classification\n",
    "\n",
    "# # ========== TRANSFORMS & MODEL ==========\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# resnet = models.resnet50(pretrained=True)\n",
    "# resnet.fc = torch.nn.Identity()\n",
    "# resnet.eval()\n",
    "\n",
    "# # ========== EMBEDDING FUNCTION ==========\n",
    "# def get_face_embedding(face_image):\n",
    "#     img = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "#     input_tensor = transform(img).unsqueeze(0)\n",
    "#     with torch.no_grad():\n",
    "#         embedding = resnet(input_tensor).squeeze(0).numpy()\n",
    "#         embedding = embedding / np.linalg.norm(embedding)\n",
    "#     return embedding\n",
    "\n",
    "# # ========== FACE DETECTION (placeholder) ==========\n",
    "# def detect_faces(frame):\n",
    "#     # Dummy HaarCascade — replace with YOLOv12 later\n",
    "#     face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     boxes = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "#     return boxes  # (x, y, w, h)\n",
    "\n",
    "# # ========== TRAIN CLASSIFIERS ==========\n",
    "# def train_binary_classifiers(embedding_file=EMBEDDING_FILE, model_dir=CLASSIFIER_DIR):\n",
    "#     os.makedirs(model_dir, exist_ok=True)\n",
    "#     with open(embedding_file, 'rb') as f:\n",
    "#         embeddings = pickle.load(f)\n",
    "\n",
    "#     names = list(embeddings.keys())\n",
    "#     X = np.array(list(embeddings.values()))\n",
    "#     for i, name in enumerate(names):\n",
    "#         y = np.array([1 if j == i else 0 for j in range(len(names))])\n",
    "#         clf = LogisticRegression(max_iter=1000)\n",
    "#         clf.fit(X, y)\n",
    "\n",
    "#         model_path = os.path.join(model_dir, f\"{name}_clf.pkl\")\n",
    "#         with open(model_path, 'wb') as f_out:\n",
    "#             pickle.dump(clf, f_out)\n",
    "#         print(f\"[TRAINED] Classifier for {name} → {model_path}\")\n",
    "\n",
    "# # ========== FACE RECOGNITION ==========\n",
    "# def recognize_face_binary(embedding, model_dir=CLASSIFIER_DIR, threshold=THRESHOLD):\n",
    "#     best_match = \"Unknown\"\n",
    "#     best_score = threshold\n",
    "\n",
    "#     for fname in os.listdir(model_dir):\n",
    "#         if fname.endswith('_clf.pkl'):\n",
    "#             name = fname.replace('_clf.pkl', '')\n",
    "#             with open(os.path.join(model_dir, fname), 'rb') as f:\n",
    "#                 clf = pickle.load(f)\n",
    "#             score = clf.predict_proba([embedding])[0][1]\n",
    "#             if score > best_score:\n",
    "#                 best_score = score\n",
    "#                 best_match = name\n",
    "#     return best_match\n",
    "\n",
    "# # ========== OPTIONAL: RUN ONCE ==========\n",
    "# # Uncomment this if you haven’t trained yet\n",
    "# # train_binary_classifiers()\n",
    "\n",
    "# # ========== REAL-TIME WEBCAM LOOP ==========\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# print(\"Starting face recognition... Press 'q' to quit.\")\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Mirror the frame\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "\n",
    "#     boxes = detect_faces(frame)\n",
    "#     for (x, y, w, h) in boxes:\n",
    "#         # Expand box slightly\n",
    "#         x1 = int(x - 0.02 * w)\n",
    "#         y1 = int(y - 0.05 * h)\n",
    "#         x2 = int(x + w + 0.02 * w)\n",
    "#         y2 = int(y + h + 0.05 * h)\n",
    "#         x1, y1 = max(0, x1), max(0, y1)\n",
    "#         x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "\n",
    "#         face_crop = frame[y1:y2, x1:x2]\n",
    "#         embedding = get_face_embedding(face_crop)\n",
    "#         name = recognize_face_binary(embedding)\n",
    "\n",
    "#         # Display\n",
    "#         cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "#         cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)\n",
    "\n",
    "#     cv2.imshow(\"Face Recognition\", frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea9f60-1a63-43ae-bd17-37a22e5e2708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
